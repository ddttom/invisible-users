# Chapter: The Invisible Buyer

*A case study in AI-mediated commerce and the need for Machine Experience*

> **Note:** The transaction described in this chapter is fictionalised, but the scenario is entirely plausible. AI assistants can and do hallucinate product features, and the resulting commercial disputes are already occurring. The stakeholder responses explored here represent realistic perspectives on a systemic challenge that grows as AI-mediated commerce expands. The specific details are illustrative; the pattern is real.

---

## The Transaction

A consultant needed a laptop. The requirements were specific: a Snapdragon processor for battery efficiency, built-in 5G connectivity for mobile working, and an HDMI port for presentations. No brand preference. No retailer loyalty. Just a functional specification expressed in plain language.

Rather than visiting retailer websites, browsing product categories, or clicking through comparison tools, the consultant described these requirements to an AI assistant. Within fifteen minutes, the assistant had searched current options, filtered by requirements, identified a winner, found UK pricing across multiple suppliers, and surfaced the key constraints.

The consultant approved the recommendation and completed the purchase. A business laptop, £746, from a trade supplier.

The laptop arrived three days later. It didn't have 5G.

The AI had hallucinated the feature. The retailer's specification was accurate – the base model didn't include 5G connectivity. The 5G variant cost significantly more and had to be specified at purchase. But the AI, synthesising information from multiple sources, had confidently stated that the recommended model included 5G. It didn't.

The consultant knew the 14-day cancellation right and acted quickly. The laptop went back. The refund cleared. Case closed – for one consumer, on one transaction.

But the incident revealed something larger: a systemic problem affecting every participant in the e-commerce ecosystem. And each participant saw it differently.

---

## What the Retailer Saw

The retailer saw a transaction appear from nowhere.

No browse history preceded it. No abandoned cart. No email signup. No engagement with their carefully constructed product pages, comparison tools, or recommendation engines. Just an order, arriving as if from thin air.

Then, days later, a return. The customer claimed the product lacked a feature they'd expected. The retailer checked their listing – accurate. Checked their specification sheet – accurate. Checked their product data feed – accurate. They had done nothing wrong.

Yet they processed a return they hadn't caused. They absorbed logistics costs for a shipment that never should have happened. They re-stocked a product that had been correctly described but incorrectly recommended by a system they didn't control.

This pattern was becoming familiar. Orders with no preceding journey. Returns with no apparent cause. Customers who seemed to know exactly what they wanted but were somehow wrong about what they'd bought.

The retailer had invested years in digital customer experience: analytics tracking every click, remarketing following abandoned carts, personalisation engines learning preferences, chatbots answering questions. All of it bypassed. The customer journey had moved somewhere they couldn't see – into conversations with AI assistants that visited their product data but never their product pages.

The economics were uncomfortable. AI-mediated customers converted at higher rates (they arrived knowing what they wanted) but returned at higher rates (when what they wanted didn't match what the AI had described). The net effect was unclear, but the returns were real costs with no obvious remedy.

---

## What the Search Engine Saw

A major search engine provider saw the query funnel compress.

Historically, a considered purchase like a laptop generated a predictable sequence: "best business laptop," then "snapdragon laptop battery life," then brand comparisons, then specific model reviews, then pricing queries, then finally a purchase search. Six or more queries, fifteen or more clicks, a journey that could be observed, measured, and monetised.

Now users were arriving at the final query directly. They knew exactly what they wanted. They were searching only to find where to buy it. The research had happened elsewhere – in conversations with AI assistants that generated no queries, showed no ads, captured no data.

The search provider estimated that AI-mediated research now represented 8-12% of considered purchases in categories like electronics. The percentage was growing quarterly. Each percentage point represented research queries that didn't happen, shopping ads that weren't shown, analytics data that wasn't captured, revenue that wasn't generated.

The search provider was adapting – integrating AI into their own search experience, providing AI-generated overviews, encouraging structured data that their systems could process. But third-party AI assistants were different. They consumed data from the web the search provider indexed, synthesised it, delivered recommendations – and the search provider saw nothing until the user arrived with a transactional query and no preceding context.

The relationship between AI assistants and the broader ecosystem was unclear. Were they symbiotic, adding value while consuming resources fairly? Or extractive, harvesting information without contributing to the infrastructure that created it?

---

## What the Lawyer Saw

A commercial solicitor saw a liability vacuum.

The consumer had relied on a representation about the product. The representation was false. The consumer suffered a loss (or at least an inconvenience). Under normal circumstances, the path to remedy was clear: the party who made the false representation bears responsibility.

But who made the representation?

The retailer's description was accurate. They never claimed the laptop had 5G. Their product data explicitly showed the connectivity options for each variant. They had fulfilled their legal obligations to provide accurate pre-contract information.

The AI assistant made the false statement. But the AI wasn't a legal person. It couldn't be sued. It couldn't pay damages. It existed as software operated by a company in another jurisdiction, insulated by terms of service that disclaimed accuracy and liability.

The consumer made a decision based on information they reasonably believed to be reliable. They weren't negligent – they used a tool that millions of people use daily for exactly this purpose. The AI presented its recommendation with confidence, not caveats.

Current law offered no clear answer. The Consumer Rights Act required goods to match their description – but whose description? The retailer's description was accurate. Contract law required meeting the terms agreed – but the terms the retailer offered were met. Misrepresentation law required a false statement by a party to the transaction – but the AI wasn't a party.

The case resolved through the 14-day cooling-off period, not through liability determination. The consumer exercised a statutory right that exists precisely because distance purchasing carries inherent risks. But that resolution left the underlying question unanswered: when AI intermediaries introduce errors into commercial transactions, who bears responsibility?

If the consumer had missed the 14-day window, the answer would have mattered. And increasingly, consumers were missing that window – discovering discrepancies only after the cancellation period expired, or not realising that the AI's confident recommendation had been fabricated.

---

## What the Government Saw

A regulator saw a gap in the consumer protection framework.

The Consumer Contracts Regulations and the Consumer Rights Act had served consumers well for over a decade. They established clear rights: pre-contract information requirements, cooling-off periods, remedies for non-conforming goods. They balanced consumer protection with commercial certainty. They were well understood by traders and consumers alike.

But they assumed a bilateral relationship between trader and consumer. They didn't account for AI intermediaries that could influence purchasing decisions while bearing no responsibility for errors.

The regulator considered extending liability. If AI providers recommended products and those recommendations proved false, perhaps AI providers should share responsibility for the resulting harm. This would create incentives for accuracy and close the liability gap.

But extending liability had problems. AI providers were often based outside UK jurisdiction – enforcement would be slow, expensive, and uncertain. Defining what constituted a "hallucination" versus a reasonable inference from ambiguous data would be legally complex. And liability frameworks addressed harm after it occurred; they compensated victims but didn't prevent the harm from happening.

The regulator had powers to intervene. They could issue guidance, propose legislation, coordinate with international counterparts. But what intervention would actually help? Extending liability might create bureaucracy without improving outcomes. Mandating accuracy might stifle innovation. Requiring disclaimers might just add friction that consumers would ignore.

The framework needed updating, but how to update it wasn't obvious.

---

## What the Credit Card Provider Saw

A credit card issuer saw an unrecoverable cost.

Section 75 of the Consumer Credit Act made card providers jointly liable with suppliers for misrepresentation on purchases between £100 and £30,000. This was valuable consumer protection – it gave cardholders confidence that credit card purchases carried additional security.

But Section 75 required misrepresentation by the supplier. If the supplier's description was accurate, there was no misrepresentation by the supplier. The AI's hallucination wasn't covered.

The card provider could decline the claim on legal grounds. But declining meant an unhappy cardholder – someone who had used their credit card partly because of Section 75 protection and now felt that protection had failed them. The relationship damage might exceed the claim value.

The card provider could pay the claim as a goodwill gesture. But paying meant absorbing a cost they hadn't caused and couldn't recover. They couldn't pursue the retailer (who had done nothing wrong) or the AI provider (who had no contractual relationship with them and was probably in another jurisdiction anyway).

Either way, processing the claim consumed resources. Investigation, documentation, decision, communication – each step had costs even when the claim was declined.

The card provider tracked these claims as a new category: "AI-mediated disputes." The volume was small but growing. If the trajectory continued, the cumulative cost would be significant – millions annually across the portfolio, with no recovery path and no clear way to prevent future occurrences.

---

## The Pattern

Each stakeholder experienced the same transaction differently, but they shared a common problem: an intermediary layer had changed, and their systems hadn't adapted.

The retailer's analytics couldn't see research that happened in AI conversations. Their product data reached the AI, but their product experience didn't reach the customer. The search engine's query funnel, built over decades, was being bypassed entirely. Their advertising model depended on observing intent; AI assistants made intent invisible. The legal framework assumed bilateral transactions with clear chains of representation. AI created trilateral transactions with a gap in the middle where errors could occur but liability couldn't attach. The regulatory framework protected consumers from trader misconduct. It didn't address misconduct by AI systems that traders didn't control. The financial services framework insured against supplier failures. It didn't insure against AI hallucinations that suppliers couldn't prevent.

Everyone was dealing with consequences they hadn't caused. No one could solve the problem individually. And the problem was growing – AI-mediated commerce was expanding, hallucination rates weren't falling to zero, and the costs were accumulating across the ecosystem.

---

## The Wrong Question

Initially, each stakeholder asked the same question: who should bear the liability?

The retailer argued they shouldn't bear costs for AI errors. Their data was accurate. They weren't responsible for how AI systems interpreted that data.

The search engine argued AI providers were extracting value without contributing. They consumed information and delivered recommendations without the accountability that search advertising required.

The lawyer argued someone must be liable. Consumer protection couldn't function if consumers had no avenue for redress when they were harmed by false information.

The regulator considered extending liability to AI providers. If they caused harm, they should bear responsibility.

The card provider prepared to absorb costs while documenting the trend, hoping regulators would eventually create frameworks that shifted costs elsewhere.

But liability is a reactive question. It asks: after harm occurs, who pays? It doesn't ask: how do we prevent harm from occurring?

And prevention, in this case, was possible.

---

## The Right Question

The AI hallucinated 5G capability because the product data was ambiguous about what the laptop lacked.

The retailer's specification accurately listed what the base model included. It didn't explicitly state what the base model excluded. The AI, encountering silence on 5G connectivity, inferred – incorrectly – that the feature was present. Or perhaps it confused specifications across variants. Or perhaps it synthesised information from multiple sources and introduced errors in the synthesis.

The exact mechanism didn't matter. What mattered was the input: data that required inference rather than data that stated facts explicitly.

Consider an alternative. If the retailer's product data had included explicit negative declarations – structured, machine-readable statements that the base model did not include 5G connectivity – the AI couldn't have hallucinated the feature. The data would have said: `wwan_5g: false`. No inference required. No ambiguity to resolve. No hallucination possible.

This is the insight at the core of Machine Experience: AI systems don't need persuasion or visual hierarchy or emotional design. They need clear, structured, unambiguous data. They need explicit statements of what is true – and explicit statements of what is not true. They need facts, not inferences.

The retailer could have prevented this dispute. Not by controlling the AI (which they couldn't) or by changing consumer behaviour (which they couldn't) or by lobbying for regulatory change (which would take years). By providing better data. Data that explicitly declared what features products had and didn't have. Data that AI systems could read without guessing.

The cost of providing such data would have been modest – a few days of work to audit specifications and add explicit negatives. The cost of the dispute – returns processing, investigation time, relationship damage, ecosystem friction – far exceeded that investment.

The right question wasn't "who pays when AI hallucinates?" The right question was "how do we structure data so AI can't hallucinate?"

---

## The Shift

When stakeholders encountered this reframing, their thinking changed.

The regulator reconsidered the policy approach. Extending liability to AI providers would be complex to implement, difficult to enforce across jurisdictions, and would address symptoms rather than causes. But encouraging structured data standards was feasible, enforceable within UK jurisdiction, and would prevent harm rather than just compensate for it. The regulator began developing voluntary codes and considering procurement requirements that would make structured data commercially advantageous.

The credit card provider reconsidered the business model. Instead of passively absorbing dispute costs, they could actively encourage merchants to provide MX-compliant data. Compliance could be a factor in risk assessment, interchange pricing, and merchant services terms. Merchants with better data would generate fewer disputes; everyone would benefit. The card provider began piloting compliance assessments and exploring data infrastructure partnerships.

The retailer reconsidered the investment priority. Years of spending on human-facing digital experience had created sophisticated websites that AI assistants bypassed entirely. Investing in machine-facing data – structured specifications, explicit declarations, machine-readable truth – would serve the customers they couldn't see. The retailer began auditing product data and implementing explicit negatives.

The search engine observed that structured data practices they'd advocated for years were now more important than ever. Schema.org markup, product feeds, knowledge graph integration – these weren't just search optimisation techniques. They were the foundation for accurate AI representation. The search engine intensified its structured data evangelism while building AI capabilities of its own.

Each stakeholder arrived at the same conclusion from different starting points: prevention was more effective than remediation, and prevention required better data.

---

## The Framework

Machine Experience provides the framework for that better data.

The core principles are straightforward:

**Explicit declaration over implicit inference.** Don't leave AI systems to guess what products have or lack. State it explicitly. If a product doesn't have 5G, say so: `wwan_5g: false`. If it doesn't have an HDMI port, say so: `hdmi_port: false`. Explicit negatives block hallucination; silence invites it.

**Structured data over natural language.** Natural language is ambiguous. "Connectivity options include WiFi 6E and Bluetooth 5.3" doesn't say whether 5G is present or absent. Structured data is precise. A JSON specification with explicit fields for each connectivity type leaves no room for misinterpretation.

**Machine readability over human readability.** Product pages are designed for human consumption: images, descriptions, reviews, calls to action. AI assistants don't experience any of that. They parse data. The data layer – structured, consistent, complete – is what AI actually sees. Investing in that layer serves the customers who research via AI.

**Prevention over remediation.** Every hallucination prevented is a return avoided, a dispute eliminated, a relationship preserved. The cost of prevention (better data) is far lower than the cost of remediation (returns, disputes, litigation, regulatory action). Prevention is the better investment.

These principles translate into practical implementation across four levels:

**Level 1: Basic machine readability.** Implement Schema.org markup for products. Provide JSON-LD specifications. Ensure product data is parseable by AI systems. This is table stakes – the minimum required to participate in AI-mediated commerce.

**Level 2: Hallucination resistance.** Add explicit negative declarations for commonly queried features. If customers frequently ask about 5G and your product doesn't have it, say so explicitly. Audit specifications for ambiguity. Remove opportunities for inference.

**Level 3: AI agent optimisation.** Provide APIs for AI systems to query product data directly. Enable real-time inventory and pricing checks. Support the agentic commerce workflows that are emerging – AI assistants that don't just recommend but negotiate and purchase.

**Level 4: Full compliance.** Comprehensive MX implementation with ongoing maintenance, canonical source marking, disambiguation protocols, and continuous improvement based on AI interaction patterns.

Most organisations will find the greatest return at Level 2. The investment is modest (weeks, not months). The impact is significant (hallucinations blocked, disputes prevented). The payback is rapid (often within a single quarter).

---

## The Conclusion

The invisible buyer never became visible to the retailer until the transaction was complete. The research journey – requirements gathering, option evaluation, comparison, decision – happened in a conversation the retailer couldn't see, on a platform they didn't control, mediated by an AI that hallucinated a feature that didn't exist.

The consumer got their money back. The 14-day cancellation right worked as designed. But the incident exposed fault lines across the entire ecosystem: retailers blind to their customers, search engines bypassed entirely, legal frameworks with liability gaps, regulators considering interventions, financial services absorbing costs.

Each stakeholder initially asked who should bear liability. That question leads to litigation, regulation, and cost-shifting – outcomes that help no one and take years to resolve.

The better question is how to prevent the harm. That question leads to Machine Experience: structured data, explicit declarations, machine-readable specifications that AI systems can interpret without guessing.

The invisible buyer isn't going away. AI-mediated commerce will only grow. The choice facing every organisation is adaptation or irrelevance.

Adaptation means recognising that the customer journey has changed – that the data layer is the new storefront, that structured specifications are the new shop window, that explicit declarations are the new sales conversation. It means investing in machine experience with the same seriousness previously reserved for human experience.

The framework exists. The implementation path is clear. The economics favour prevention. The only question is how quickly organisations will act.

The consultant who needed a laptop learned something from fifteen minutes of AI conversation and ten days of return processing. The lesson applies to every participant in e-commerce: when machines mediate between buyers and sellers, the experience you design for machines determines the outcomes you get.

That's why Machine Experience matters. That's why this discipline exists.

And that's why the books were written.

---

*This chapter is adapted from "The Invisible Buyer" blog series, which explored a single fictionalised transaction from eleven perspectives. The full series is available at [website].*

*Tom Brennan is founder of the Machine Experience (MX) community and Principal Consultant at Digital Domain Technologies Ltd. He has worked in technology for 52 years.*
