Adobe Experience Cloud Team
Regarding: LLM Optimizer and Generative Engine Optimization Strategy

---

Dear Adobe Team,

I'm reaching out to share some thoughts on LLM Optimizer and the broader approach to AI agent compatibility. I have great respect for Adobe's products and your team's innovation in addressing this emerging challenge. LLM Optimizer solves real, urgent problems for enterprise clients who need immediate solutions.

## What LLM Optimizer Gets Right

Your pre-rendering capability addresses a genuine technical constraint: AI agents fundamentally cannot execute JavaScript the way browsers do. For JavaScript-heavy architectures where server-side rendering isn't practical, edge-based pre-rendering solves a problem that content improvements alone cannot fix.

This is legitimate and valuable. You've built a practical tool that delivers immediate results.

## Where I See a Strategic Opportunity

My concern isn't with the pre-rendering technology - it's with positioning edge-based content improvement (simplified paragraphs, improved headings, generated summaries) as the primary solution rather than a transitional tool.

Here's the fundamental question: if content needs simpler paragraphs, clearer headings, and better summaries for AI agents, doesn't it need those things for humans too?

## Pattern I'm Seeing

When LLM Optimizer suggests content improvements, those suggestions reveal underlying content quality issues. Sites implementing these changes only at the edge are:

1. **Maintaining two versions** - One optimised, one not
2. **Missing the broader opportunity** - Better content benefits everyone
3. **Creating technical debt** - Edge-based content fixes become permanent workarounds

**The convergence principle**: Patterns that help AI agents also help humans. When you simplify a paragraph for an agent, you're making it more scannable for humans. When you improve heading structure, you're helping screen readers and search engines too.

## Why This Problem is Invisible

I refer to AI agents as "invisible users" for two reasons, and this matters for Adobe's enterprise clients.

First, agents are invisible to site owners. Unless you're specifically tracking agent traffic - and most aren't - you have no idea how many agents visit your site or whether they succeed at their tasks. They blend into analytics as slightly unusual sessions: short visits, no scrolling, rapid form completion, then gone.

Your clients' analytics look normal. Session duration is fine. Bounce rate is acceptable. Conversion rate hasn't changed dramatically. But revenue is down, and they can't figure out why.

Here's what's happening: AI agents are visiting their sites, extracting content, answering user questions, and sending those users to competitors. They never see these visits in analytics. They never see the lost conversions. They just see the revenue gap.

Second, the interface is partly invisible to agents. They can't see beautiful animations. They miss three-second toast notifications. They don't understand that a loading spinner indicates "wait" or that a greyed-out button indicates "not available". They experience a stripped-down, confusing version of carefully designed sites.

That elegant single-page application where content updates seamlessly without changing URLs? The agent clicked a button, waited, saw the same URL, and had no way to know if something happened or if there's an error.

**This is why edge-based content improvement alone might not solve the fundamental problem.** If the underlying interface patterns confuse agents, simplified content won't fix navigation failures, form submission uncertainties, or state management ambiguities.

## My Current Work

I'm writing "The Invisible Users: Designing the Web for AI Agents and Everyone Else" (publication Q1 2026). The book argues for fixing root causes rather than treating symptoms:

- **Structured data (Schema.org JSON-LD)** - Makes content machine-readable for agents and search engines
- **Semantic HTML** - Benefits accessibility, SEO, and agent parsing
- **Clear information architecture** - Explicit pricing, complete specifications, unambiguous state
- **llms.txt files** - Guides agents to high-value content (similar to robots.txt)

The blog post "Fix Your Content, Not Your CDN" examines LLM Optimizer as a case study. The piece is respectful but direct about the strategic implications.

## Adobe Opportunity

Adobe can uniquely lead this space, not just with edge-based fixes but with native CMS capabilities for agent-compatible content:

1. **Schema.org integration** - First-class structured data generation in Adobe Experience Manager
2. **Semantic HTML templates** - Default to proper information architecture
3. **Content quality analysis** - Flag content issues during authoring, not just at deployment
4. **llms.txt generation** - Automated discovery files based on content strategy

**Competitive advantage**: If Adobe Experience Cloud made it trivial to publish agent-compatible content from the CMS, edge-based improvement becomes the fallback for legacy architectures, not the primary approach.

## Real-World Impact

I've seen the Tailwind CSS case (January 2025, 75% staff reduction). Their revenue model depended on documentation traffic leading to paid product discovery. When agents started answering questions without sending traffic, the business collapsed.

Their mistake wasn't technical - it was strategic. Free documentation was perfectly discoverable. Paid products remained invisible to agents. They needed Schema.org Product markup, llms.txt directing agents to commercial pages, and pricing visible in served HTML.

Edge improvement wouldn't have solved that. Content strategy would have.

## Why This Matters for Adobe

Your enterprise clients face the same pressures:

- Traffic-driven advertising revenue declining
- Affiliate marketing click-through collapsing
- "Freemium" models where free content drives awareness but paid offerings stay invisible

LLM Optimizer helps with content visibility. But it doesn't solve the business model transformation required when AI agents extract free content whilst revenue opportunities go uncited.

## The Timeline Compressed

The market moved faster than anyone anticipated. When work on AI agent commerce started in 2024, it felt like an emerging concern with 12-18 months runway for preparation.

December 2025: Microsoft announced Copilot Checkout, signalling that agent commerce had moved from experiment to platform strategy.

January 2026: Three major platforms launched agent commerce systems within seven days. Amazon's Alexa.com (January 5th), Microsoft's Copilot Checkout expansion (January 8th), Google's Universal Commerce Protocol with Target and Walmart (January 11th).

The timeline compressed from projected 12-18 months to weeks. Your enterprise clients who thought they had time to "see how things develop" are already behind competitors implementing these patterns now.

## Platform Trust and the Compounding Effect

Here's what most businesses don't understand yet: platforms act as gatekeepers. Google, OpenAI, Anthropic, Perplexity - their agents only send traffic to sites that have high success rates for automated tasks.

If agents consistently fail on a site - timeout errors, confusing structure, missing information - platforms deprioritise it in citations. Why would ChatGPT cite a restaurant it can't extract menu information from? Why would Claude reference a product page where the price is ambiguous?

But if agents consistently succeed - accurate extraction, clear transaction paths, reliable structured data - platforms trust that site. They cite it more frequently. They send users with confidence.

**This creates a compounding effect.** Successful agent interactions make future citations more probable. Failed interactions reduce subsequent citation likelihood. The first businesses in each sector to become genuinely agent-friendly gain a platform trust advantage that becomes harder for competitors to overcome.

**Edge-based fixes might not build this trust.** If simplified content is served to agents whilst underlying navigation patterns remain confusing, agents still fail at completing tasks. The content reads well, but the transaction fails. The platform learns: "This site looks good but doesn't work reliably."

Native CMS implementation of agent-compatible patterns builds genuine platform trust because the entire experience works, not just the content layer.

## Diplomatic Way Forward

I'm not suggesting you abandon LLM Optimizer. I'm suggesting you position it differently:

**Current positioning**: "Improve content for AI agents at the edge"
**Strategic positioning**: "Immediate fixes whilst you implement complete GEO"

The fundamental shift your clients face: traditional SEO focused on ranking to capture clicks. GEO (Generative Engine Improvement) focuses on citations in AI-generated answers. The click disappears. The citation replaces it.

When users ask ChatGPT or Claude for recommendations, agents cite sources based on what they found during research. If structured data is missing, pricing is ambiguous, or navigation patterns are confusing, your clients don't get cited. Users never know they exist.

The patterns that improve citations also improve traditional search rankings. Structured data, semantic HTML, clear information architecture - these work across the entire discovery ecosystem. This is why native CMS implementation provides sustained advantage whilst edge-based fixes remain transitional.

Use LLM Optimizer's insights diagnostically. When the tool suggests simplified paragraphs or improved headings, guide clients to fix the underlying content in their CMS so everyone benefits.

Position Adobe Experience Cloud as the platform for native agent compatibility, not just edge-based workarounds.

## Collaboration Opportunities

I'd welcome the chance to discuss this further:

1. **Book contribution** - Adobe's perspective on enterprise GEO challenges
2. **Joint research** - Measuring ROI of edge-based vs native content improvements
3. **Product feedback** - Specific features that would support the convergence principle
4. **Speaking opportunities** - Adobe Summit, Experience Makers, partner events

I've built a Web Audit Suite (Node.js) that implements these patterns. It crawls sites, analyses agent compatibility, generates detailed reports. It's production-ready and could complement LLM Optimizer as a diagnostic tool.

## My Background

- Digital Domain Technologies Ltd
- 20+ years software engineering and web architecture
- Specialising in AI agent compatibility and web accessibility
- Author, "The Invisible Users" (forthcoming Q1 2026)

**Contact:**

- Email: <tom.cranstoun@gmail.com>
- LinkedIn: <https://linkedin.com/in/tomcranstoun>
- Website: <https://allabout.network>

## Closing Thoughts

Adobe has consistently led through innovation that anticipates market shifts. LLM Optimizer proves you saw this shift coming. My hope is that Adobe will lead the next phase too - not just adapting to AI agents, but defining how enterprise content management evolves for this new reality.

The convergence principle isn't criticism of your tools. It's an opportunity to expand what those tools enable.

I respect the engineering that went into LLM Optimizer. The pre-rendering technology is well-engineered. My concern is strategic: edge-based content improvement appears to be the right solution for 2026 that becomes technical debt by 2028.

We can build the future where content works for everyone - humans, search engines, and AI agents - through better information architecture, not CDN workarounds.

I'd welcome your thoughts.

Best regards,
Tom Cranstoun
Digital Domain Technologies Ltd

---

## Appendix: Resources

Blog post: "Fix Your Content, Not Your CDN"
Location: packages/manuscript/the-bible-of-mx/blog/geo-blog.md
Summary: Examines LLM Optimizer as case study, argues for fixing root causes

Book: "The Invisible Users: Designing the Web for AI Agents and Everyone Else"
Publication: Q1 2026
Coverage: Complete GEO patterns, Schema.org implementation, testing workflows

Web Audit Suite: Node.js tool analysing agent compatibility
Features: Sitemap crawling, Schema.org validation, accessibility testing, LLM suitability metrics
Status: Production-ready, available for partnership discussions

---

Note: This document is part of a larger sales enablement package. For Adobe team members encountering this, please feel free to reach out directly. I'm genuinely interested in collaboration, not competition.
