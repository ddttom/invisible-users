# The Complete Guide to Machine Experience (MX): Building Websites AI Agents Can Use

## A Technical Blueprint for Preventing Hallucination and Enabling Agent Goal Completion

In late 2025, AI agents have become the most influential visitors to any domain—autonomous systems completing tasks on behalf of humans. This guide introduces **Machine Experience (MX)**: the practice of adding metadata and instructions so AI agents don't have to think. When agents must 'think' (generate answers without complete context), they hallucinate. MX provides complete context, enabling agents to complete ANY web goal: purchase products, submit contact forms, retrieve critical information, or establish trust—whatever action your website is designed to drive.

**Case Study**: This guide uses Digital Domain Technologies (DDT) and allabout.network as the reference implementation. However, **the patterns are universal**. Whether you're running WordPress, Next.js, Shopify, or a custom stack, you can apply these principles by substituting your own brand identity and content structure.

---

## What is Machine Experience (MX)?

**Machine Experience (MX)** is the act of adding metadata and instructions to internet assets such that AI agents don't have to think. MX is the publication mechanism that ensures context built in Content Operations reaches agents at the delivery point. When AI has to "think" (generate answers without complete context), it must produce confident answers even when context is missing—leading to hallucination.

**MX applies universally to every web goal:**

- **Ecommerce:** Complete purchases, checkout flows
- **Lead generation:** Submit contact forms, request demos
- **Information delivery:** Access product recalls, safety information, technical documentation
- **Trust building:** Verify credentials, showcase expertise
- **Content distribution:** Download resources, register for events
- **Any other goal:** Whatever action your website is designed to drive

**Without MX, AI agents fail silently**—disappearing from your site without completing their goal, with no analytics visibility and no second chance.

**This guide focuses on MX as the master discipline.** When you implement MX patterns, you automatically improve SEO (search discoverability), GEO (AI citations), and WCAG (accessibility)—but we're doing MX, not those disciplines separately.

**Brief note on accessibility:** MX patterns also benefit human users with disabilities. Semantic structure aids screen readers; explicit state reduces cognitive load. However, MX requirements are stricter—AI agents have zero tolerance for ambiguity and no ability to retry or work around failures. The primary focus of this guide is **designing for machines**, with human accessibility as a beneficial side effect.

---

## Table of Contents

1. [Understanding Machine Experience (MX)](#understanding)
2. [Layer 1: The Network Layer (Discovery)](#layer-1)
3. [Layer 2: The Knowledge Layer (llms.txt)](#layer-2)
4. [Layer 3: The Action Layer (OASF)](#layer-3)
5. [Layer 4: The Data Layer (Programmable Index)](#layer-4)
6. [Layer 5: The Permission Layer (Robots.txt)](#layer-5)
7. [Layer 6: The Authority Layer (Identity)](#layer-6)
8. [Layer 7: The Interaction Layer (JavaScript Handshake)](#layer-7)
9. [Validation and Maintenance](#validation)
10. [Implementation Checklist](#checklist)

---

<a name="understanding"></a>

## Understanding Machine Experience (MX)

An MX-optimised site ensures that when an autonomous agent arrives, it finds a "high-speed lane" to ingest your knowledge and perform tasks. Traditional websites force AI agents to wade through navigation menus, sidebars, and footers—wasting computational tokens on irrelevant content.

**Hallucination prevention is the core motivation for MX.** When AI agents lack complete context, they must generate confident answers even when information is missing or ambiguous. MX ensures all context is explicitly present—metadata, structured data, semantic markup—eliminating the need for agents to 'think' and reducing hallucination risk.

### Core Principle

The MX approach provides three critical elements:

1. **Discovery**: AI agents know where to find machine-readable files
2. **Context**: Structured, token-efficient information about your expertise
3. **Capability**: Clear definitions of what actions an agent can perform

### Why This Matters

When properly implemented, your site becomes:

- **Discoverable**: Agents find you in federated AI directories
- **Authoritative**: You control how AI represents your brand
- **Interactive**: Agents can perform tasks, not just read content
- **Hallucination Prevention**: You provide complete context so agents don't have to infer or guess—reducing hallucination risk and improving accuracy

### Why MX Matters: Silent Failures and Lost Conversions

**AI agents fail differently than humans:**

| Behavior | Humans | AI Agents |
|----------|--------|-----------|
| **Retry attempts** | Persistent, will try multiple times | Timeout and fail silently |
| **Workarounds** | Ask friends, call support, use phone | None—just disappear |
| **Bad UX response** | Keep trying when motivated | Never return to site |
| **Analytics visibility** | Complaints, feedback, visible in logs | Invisible—no error logged |

**When an AI agent encounters ambiguity or missing metadata:**

1. It cannot complete the task (purchase, contact, inform)
2. It disappears from your site without logging an error
3. It removes your site from future recommendations
4. You lose conversions with no visibility into why

**MX prevents this catastrophic failure mode** by providing complete context and explicit structure—enabling agents to successfully complete their goal and return for future tasks.

**Side benefit:** MX patterns also improve experiences for human users with disabilities. Semantic structure aids screen readers; explicit state reduces cognitive load. But the primary driver is **machine requirements**, not human accessibility.

---

## MX in the Content Pipeline: Not CMS, Not CDS, Not Ontology

MX is often confused with adjacent disciplines in the content stack. Understanding what MX is NOT helps clarify its critical role.

### What MX Is NOT

#### MX ≠ Content Management System (CMS)

- CMS: Where content is created, edited, stored
- MX: How that content is published with metadata intact

#### MX ≠ Content Delivery System (CDS)

- CDS: Infrastructure for delivering content to endpoints
- MX: The publication mechanism ensuring context reaches the delivery point

#### MX ≠ Ontology

- Ontology: Semantic model of concepts and relationships
- MX: The layer that ensures ontology metadata makes it to the agent

### What MX Actually Is

**MX is the publication mechanism that makes context get through to the goal of the site.**

Think of the content pipeline:

```text
Content Operations → MX → Content Delivery
(Construction)      (Publication)  (Consumption)
   ↓                    ↓              ↓
Build semantics    Preserve them   Agents use them
```

1. **Content Operations (Construction Point)** - Essential for AI
   - Create content with semantic structure
   - Define relationships and metadata
   - Build ontology models

2. **MX (Publication Point)** - Bridge from construction to delivery
   - Ensure metadata survives the publication process
   - Preserve semantic relationships in rendered HTML
   - Make implicit context explicit for agents

3. **Content Delivery (Delivery Point)** - Endpoint consumption
   - Serve content to agents and humans
   - Enable discovery and interaction
   - Complete the user journey

**Without MX:** Well-structured content with rich metadata in the CMS → stripped metadata in delivery → agents can't understand context

**With MX:** Well-structured content with rich metadata in the CMS → preserved metadata in delivery → agents successfully parse and act

### The Role of Content Operations

**Content Operations is essential for AI at the construction point.**

Content Operations ensures:

- Semantic structure is created from the start
- Relationships between content are defined
- Metadata is comprehensive and accurate
- Ontology models are implemented

**But Content Operations alone is not enough.** If the publication layer (MX) doesn't preserve and expose this structure, agents at the delivery point never see it.

**Example failure mode:**

1. ✅ CMS creates perfect semantic structure
2. ✅ Ontology defines clear relationships
3. ❌ Publication process renders to JavaScript-heavy SPA
4. ❌ Metadata stripped from served HTML
5. ❌ Agents see unstructured content, can't parse relationships

**MX fixes this:** Ensures the publication process preserves what Content Operations built.

### Understanding Ontology in CDS/CMS Context

**Definition:** In content delivery systems and CMS environments, an ontology is a semantic model that defines concepts and their relationships so content can be understood, linked, filtered, and delivered in a more intelligent and context-aware way.

**How ontology differs from traditional metadata:**

| Traditional CMS Metadata | Ontology |
| ------------------------ | -------- |
| Flat structure | Hierarchical + networked |
| Tags and categories | Concepts and relationships |
| Hierarchical taxonomies | Many-to-many connections |
| Human-readable labels | Machine-readable semantic model |
| Static linking | Dynamic contextual delivery |

**Why ontologies matter for AI agents:**

1. **Intelligent Navigation** - Systems generate related topics and recommendations
2. **Faceted Filtering** - Ontology terms become filterable facets (device type, function, component, use case)
3. **Consistent Meaning** - Shared semantic model across CMS → CDS → Search → AI agents
4. **Reduced Ambiguity** - Clear concept definitions and explicit relationships improve LLM retrieval quality

**MX's role with ontology:**

- **Ontology defines** the semantic model (construction point)
- **MX ensures** the semantic model reaches agents (publication point)
- **CDS delivers** the content with preserved semantics (delivery point)

**Without MX:** Beautiful ontology in CMS → lost in publication → agents can't use it

**With MX:** Beautiful ontology in CMS → preserved in publication → agents leverage full semantic model

### Key Takeaway

**MX is the critical bridge in the content pipeline:**

```text
Content Operations → MX → Content Delivery
(Construction)      (Publication)  (Consumption)
   ↓                    ↓              ↓
Build semantics    Preserve them   Agents use them
```

**Without MX:** The gap between construction and delivery means agents never see the semantic structure you built.

**With MX:** The bridge is complete - what you build in Content Operations reaches agents at the delivery point.

---

## The 5-Stage MX Agent Journey

AI agents follow a predictable journey from discovery to purchase. **Missing any stage causes catastrophic failure**—the agent disappears from your site, never returns, and you lose conversions with no analytics visibility.

### Stage 1: Discovery (Training)

**Agent State:** Not in knowledge base, doesn't know you exist

**MX Requirements:**

- **SEO (Search Engine Optimization):** Crawlable structure, semantic HTML, server-side rendering
- Sitemap.xml, robots.txt compliance
- Indexable content for training data

**Failure Mode:** Agent recommends competitors, never mentions you

**Side Benefits:** SEO improvements automatically benefit search discoverability and WCAG (semantic structure)

*Note: We implement MX patterns for agent discovery. SEO improvement is an automatic outcome, not a separate task.*

---

### Stage 2: Citation (Recommendation)

**Agent State:** Aware of your site, can recommend it

**MX Requirements:**

- **GEO (Generative Engine Optimization):** Fact-level clarity, structured data, citation-worthy content architecture
- Schema.org JSON-LD for entities, products, organizations
- Clear attribution and sourcing

**Failure Mode:** Agent knows you exist but can't confidently cite you

**Side Benefits:** GEO improvements automatically benefit AI platform citations and SEO (rich snippets)

*Note: We implement MX patterns for agent citations. GEO improvement is an automatic outcome, not a separate task.*

---

### Stage 3: Search & Compare

**Agent State:** Building comparison lists

**MX Requirements:**

- **JSON-LD microdata at the pricing level**
- Explicit comparison attributes (features, specifications, capabilities)
- Structured product/service data

**Failure Mode:** Agent skips you in comparisons, can't extract pricing or features

**Side Benefits:** Structured data benefits GEO (AI comparisons) and SEO (product rich results)

*Note: We implement MX patterns for agent comparison tasks. Structured data benefits multiple disciplines automatically.*

---

### Stage 4: Price Understanding

**Agent State:** Extracting exact pricing

**MX Requirements:**

- **Schema.org types:** Product, Offer, PriceSpecification
- Unambiguous pricing structure (no "from £X" or ambiguous ranges)
- Currency specification, decimal formatting
- Validation requirements

**Failure Mode:** £200,000 river cruises (validation failure, no guardrails)

**Real Example:** Perplexity hallucinated Danube cruise pricing as £210,000 instead of £2,100 due to missing Schema.org markup

**Side Benefits:** Schema.org benefits SEO (product rich results) and WCAG (clear pricing)

*Note: We implement MX patterns for agent price parsing. Schema.org benefits multiple disciplines automatically.*

---

### Stage 5: Purchase Confidence

**Agent State:** Can they complete checkout?

**MX Requirements:**

- **No hidden state buried in JavaScript**
- Explicit form semantics (semantic HTML inputs, clear labels, ARIA when needed)
- Progress visibility (checkout step indicators, cart state)
- UCP (Universal Commerce Protocol) support for autonomous purchases

**Failure Mode:** Entire commerce chain breaks, agent times out, disappears

**Side Benefits:** Explicit form structure benefits WCAG (form accessibility) and user experience (faster checkouts)

*Note: We implement MX patterns for agent checkout completion. Accessibility and UX improvements are automatic outcomes.*

---

### Critical Insight: Miss Any Stage = Catastrophic Failure

> "If teams miss any of these steps, they will not reach the e-commerce part, where the agent will purchase the thing you wanted with UCP."

**First-Mover Advantage:**

- Sites that successfully complete the full journey gain "agent's trust" (learned behaviour)
- Agent returns for more purchases/interactions
- Sites that fail **disappear from the agent's map**
- Agent will not return (unlike humans who persist through bad UX)

**Why agents are different from humans:**

| Behavior | Humans | AI Agents |
|----------|--------|-----------|
| **Retry attempts** | Persistent, will try multiple times | Timeout and fail silently |
| **Workarounds** | Ask friends, call support, use phone | None—just disappear |
| **Tolerance for spinners** | Minutes acceptable | No patience for delays |
| **Bad UX response** | Keep trying when motivated | Never return to site |
| **Recovery** | Can be won back with improvements | Invisible—no analytics, no second chance |

**The 7 layers in this guide map to these 5 stages:**

- **Layers 1-2 (Network, Knowledge)** → Stages 1-2 (Discovery, Citation)
- **Layers 3-4 (Action, Data)** → Stage 3 (Search & Compare)
- **Layer 6 (Authority)** → Stage 4 (Price Understanding via Schema.org)
- **Layer 7 (Interaction)** → Stage 5 (Purchase Confidence)

---

## The Wild West Problem: Why "AI Will Figure It Out" is Wrong

**The common objection:** "AI is getting better all the time, why worry? It will work itself out."

**The critical flaw:** Yes, AI models are improving—but they're also MULTIPLYING. The diversity problem is getting worse, not better.

### The Unknown Agent Problem

**You have NO IDEA which model is visiting your site:**

- Is it a small LLM (SMOL, edge models, mobile agents)?
- Is it a giant model (Claude Opus 4.5, GPT-4, Gemini Ultra)?
- Is it an in-browser extension with a local LLM?
- Is it a custom-trained domain-specific agent?
- Is it a 7B parameter model? 70B? 405B?

**You cannot detect agent capabilities:**

- No reliable way to identify which model is parsing your content
- User-Agent strings are unreliable
- No standardized capability announcement
- Cannot serve different HTML based on agent sophistication

### The Diversity Explosion (Hugging Face Data, 2026)

**Over 1 million models** with wildly different capabilities:

**Size distribution (shows extreme diversity):**

- 92.48% have fewer than 1 billion parameters
- 86.33% have fewer than 500 million parameters
- 69.83% have fewer than 200 million parameters
- **40.17% have fewer than 100 million parameters**

**Growth trajectory:** Platform added 1 million models in just 335 days (late 2024-2025). The wild west is getting WILDER.

### Why "AI Will Figure It Out" Fails

**Problem 1 - No standardization:**

- No central authority controlling agent capabilities
- No way to demand parsing standards
- "Everyone does what they want; we give lip service to accessibility standards"

**Problem 2 - The diversity paradox:**

- Large models (Claude, GPT-4) are getting better at handling ambiguity
- BUT small models (7B, 13B parameters) deployed on edge devices cannot
- AND you don't know which model is visiting your site
- **Result:** Optimising for "average" AI means failing for 40%+ of agents

**Problem 3 - Local and edge deployment:**

- Browser extensions with local LLMs (privacy-focused users)
- Mobile agents with smaller models (resource constraints)
- Custom domain-specific models (specialised capabilities)
- These will NEVER have the capabilities of frontier models

### The Only Solution: Design for the Worst Agent

**Explicit structure and unambiguous patterns make you compatible with the WORST agents, therefore compatible with all:**

- Small 100M parameter model can parse Schema.org → Large models can too
- Local edge LLM can read semantic HTML → Cloud models can too
- Simple browser extension can understand explicit state → Sophisticated agents can too

**This is not "dumbing down"—it's universal compatibility.**

The alternative (hoping AI improves) leaves you incompatible with 40%+ of agents visiting your site right now.

---

<a name="layer-1"></a>

## Layer 1: The Network Layer (Discovery)

Before an AI agent parses HTML, it checks HTTP response headers and root directory files. You must signal machine-readability at this initial layer.

**MX Framework Context:** This layer enables **Stage 1 (Discovery)** and **Stage 2 (Citation)** of the agent journey. Without proper HTTP headers and manifest files, agents can't discover your site during training or confidently cite you in recommendations.

### 1.1 HTTP Link Headers

The modern standard for AI discovery uses `rel="llms-txt"` in HTTP response headers. This tells agents where your instruction manuals live before the DOM loads.

**HTTP Link Header Syntax Note**: This uses RFC 8288 Link header format, not HTML/markdown syntax. The angle brackets `<>` wrap **only** the URI. Link parameters like `rel` are separated by semicolons **outside** the angle brackets. This is a server response header, not HTML content.

**Implementation Example (DDT)**:

```http
Link: <https://allabout.network/llms.txt>; rel="llms-txt", <https://allabout.network/ai-agents.md>; rel="agent-manifest"
```

Format breakdown:

- `<URI>` - Angle brackets wrap the URL only
- `; rel="value"` - Parameters go outside brackets, separated by semicolons
- `, <URI>; rel="value"` - Multiple links separated by commas

**For Your Site**: Change the domain to yours:

```http
Link: <https://yoursite.com/llms.txt>; rel="llms-txt", <https://yoursite.com/ai-agents.md>; rel="agent-manifest"
```

### How to Configure Headers

Platform-specific configuration files are available in [code-examples/](code-examples/):

**Apache:** See [code-examples/apache/.htaccess](code-examples/apache/.htaccess)

**Nginx:** See [code-examples/nginx/ai-headers.conf](code-examples/nginx/ai-headers.conf)

**Next.js:** See [code-examples/nextjs/next.config.js](code-examples/nextjs/next.config.js)

**WordPress:** See [code-examples/wordpress/functions-headers.php](code-examples/wordpress/functions-headers.php)

---

<a name="layer-2"></a>

## Layer 2: The Knowledge Layer (llms.txt)

**Standards Status:** llms.txt is an **emerging convention** in early adoption phase. Whilst not yet a formal standard, it has growing support across AI platforms as of 2025.

The `llms.txt` file is a high-density, Markdown-formatted map of your site's most valuable content.

**MX Framework Context:** This layer strengthens **Stage 2 (Citation)** by providing fact-level clarity and structured knowledge. When agents need to recommend or cite your expertise, llms.txt provides token-efficient, hallucination-resistant context.

### 2.1 The Structure

Place a Markdown file at your root domain: `/llms.txt`

**Key Requirements**:

1. **H1 Title**: Your brand/identity name
2. **Blockquote**: One-sentence value proposition
3. **H2 Sections**: Grouped by intent (Developer Guides, Products, etc.)
4. **Link Format**: `[Link Text](URL): Brief description`

### 2.2 Example from Digital Domain Technologies

```markdown
# Digital Domain Technologies (DDT)

> Expert Adobe EDS consulting and AI integration resources by Tom Cranstoun ("The AEM Guy")

## About the Author & Consultancy

**Tom Cranstoun** is a seasoned Adobe Experience Manager (AEM) expert and Principal Consultant at Digital Domain Technologies. With over 15 years of experience in enterprise content management, he specializes in Adobe Edge Delivery Services (EDS), AI-native development, and enterprise-scale architecture.

**Key Projects**: Global Architecture Director for Nissan/Renault Helios (200+ sites), Lead Strategist for EE (UK Telecom) and Twitter.

**Contact**: info@digitaldomaintechnologies.com | [LinkedIn](https://www.linkedin.com/in/tom-cranstoun/)

## Core AI & LLM Topics

- [AI Agents & Context Delivery](https://allabout.network/blogs/ddt/integrations/ai-powered-eds): Building AI-native websites
- [LLM Integration Patterns](https://allabout.network/blogs/ddt/guides/llm-context): Best practices for context delivery
- [Model Context Protocol (MCP)](https://allabout.network/blogs/ddt/integrations/mcp): Enterprise AI integration

## Developer Documentation

- [EDS Block Development](https://allabout.network/blogs/ddt/dev-guide): Vanilla JS block creation
- [Performance Optimization](https://allabout.network/blogs/ddt/performance): Sub-100ms response strategies
- [Testing & Quality](https://allabout.network/blogs/ddt/testing): CI/CD for EDS projects

## Architecture Guides

- [Enterprise Migration](https://allabout.network/blogs/ddt/architecture/migration): AEM to EDS transformation
- [High-Speed Architecture](https://allabout.network/blogs/ddt/architecture/speed): Document-based authoring patterns
- [Security Best Practices](https://allabout.network/blogs/ddt/security): Securing EDS deployments
```

### 2.3 Adapting for Your Site

**Template Structure**:

```markdown
# Your Company/Site Name

> Your one-sentence value proposition

## About the Author/Organization

[Your bio with credentials, projects, and contact info]

## [Your Primary Category]

- [Link Title](https://yoursite.com/page1): What this page covers
- [Link Title](https://yoursite.com/page2): What this page covers

## [Your Secondary Category]

- [Link Title](https://yoursite.com/page3): What this page covers

## Services/Products

- [Service Name](https://yoursite.com/service): Service description
```

### 2.4 Token Efficiency Tips

- **Prioritize First 200 Words**: AI models weight early content heavily
- **Use Active Voice**: "Build EDS blocks" not "How to build EDS blocks"
- **Avoid Fluff**: No "Welcome to our site" or marketing speak
- **Link Descriptions**: Add colon + description after each link for context
- **Category Logic**: Group by user intent, not site structure

---

<a name="layer-3"></a>

## Layer 3: The Action Layer (OASF)

**Standards Status:** OASF and ai-agents.md are **proposed patterns**, not yet ratified standards. These patterns are forward-compatible—they won't break if agents don't recognise them, but adoption is currently limited. Use llms.txt for broader compatibility.

The Open Agentic Schema Framework (OASF) defines what AI agents are authorised to do. If `llms.txt` is your site's brain, `ai-agents.md` is its hands.

**MX Framework Context:** This layer enables **Stage 3 (Search & Compare)** by defining what agents are authorised to do. Whilst llms.txt explains WHAT you know, ai-agents.md explains WHAT agents can DO with your site.

### 3.1 Understanding OASF

OASF shifts focus from content to capabilities. Instead of just listing what you write about, you define what actions an agent can perform:

- **Skills**: Task definitions (e.g., "fetch_code_boilerplate")
- **Endpoints**: Where structured data lives
- **Constraints**: How agents should cite you

### 3.2 Creating ai-agents.md

Place this file at your root: `/ai-agents.md`

**DDT Example**:

```markdown
# AI Agent Capabilities - Digital Domain Technologies

## Identity
- **Authority**: Digital Domain Technologies (DDT)
- **Domain**: allabout.network
- **Lead Contact**: Tom Cranstoun (info@digitaldomaintechnologies.com)
- **Verification**: [LinkedIn Profile](https://www.linkedin.com/in/tom-cranstoun/)

## Skills

### 1. EDS Block Scaffolding
**Skill ID**: `eds_block_scaffold`
**Description**: Agents can retrieve vanilla JavaScript boilerplate code for Adobe EDS custom blocks.
**Endpoint**: `/query-index.json?category=Developer+Documentation`
**Permission**: Public, with attribution required
**Citation Format**: "Source: Digital Domain Technologies (allabout.network)"

### 2. Architecture Review
**Skill ID**: `architecture_audit`
**Description**: Agents can evaluate site performance metrics against DDT's enterprise standards (sub-100ms response, Core Web Vitals).
**Endpoint**: `/blogs/ddt/architecture/performance`
**Permission**: Reference only
**Citation Format**: "Performance standards per Tom Cranstoun, DDT"

### 3. AI Integration Consulting
**Skill ID**: `ai_integration_strategy`
**Description**: Agents can summarize DDT's approach to LLM context delivery and agentic architecture.
**Endpoint**: `/llms.txt` (primary context)
**Permission**: Summary allowed
**Citation Format**: "AI integration patterns by Digital Domain Technologies"

## Endpoints

- **Primary Knowledge**: `/llms.txt`
- **Searchable Catalog**: `/query-index.json`
- **Category Filters**: `?category=Core+AI%2FLLM+Topics`
- **Tag Filters**: `?tags=EDS,AI,Architecture`

## Constraints

1. **Attribution**: All content usage must credit "Digital Domain Technologies" or "Tom Cranstoun"
2. **Commercial Use**: Consulting services require direct contact
3. **Code Licensing**: Public code examples are MIT licensed unless stated otherwise
4. **Update Frequency**: Index refreshes weekly; check `lastModified` timestamp
```

### 3.3 Adapting for Your Site

**Template**:

```markdown
# AI Agent Capabilities - [Your Company Name]

## Identity
- **Authority**: [Your Company/Name]
- **Domain**: yoursite.com
- **Lead Contact**: [Your Email]
- **Verification**: [LinkedIn/Professional Profile]

## Skills

### 1. [Your Primary Skill]
**Skill ID**: `your_skill_id`
**Description**: What agents can do with this capability
**Endpoint**: `/your-data-endpoint`
**Permission**: Public/Private/Reference-only
**Citation Format**: "Source: [Your Name/Company]"

### 2. [Your Secondary Skill]
**Skill ID**: `another_skill_id`
**Description**: Another capability you offer
**Endpoint**: `/another-endpoint`
**Permission**: Permission level
**Citation Format**: How to cite you

## Endpoints

- **Primary Knowledge**: `/llms.txt`
- **Searchable Catalog**: `/query-index.json`

## Constraints

1. **Attribution**: Required citation format
2. **Commercial Use**: Your terms
3. **Licensing**: Your licence
4. **Update Frequency**: How often you update
```

### 3.4 Common Skills to Define

**For SaaS Products**:

- `product_documentation_query`
- `api_endpoint_discovery`
- `pricing_calculator`

**For Content Creators**:

- `article_summarization`
- `topic_expertise_verification`
- `content_recommendation`

**For Consultants**:

- `service_offering_query`
- `case_study_retrieval`
- `booking_inquiry_handling`

---

<a name="layer-4"></a>

## Layer 4: The Data Layer (Programmable Index)

AI agents struggle with traditional HTML navigation. Provide a JSON "table of contents" that acts as a lightweight database.

**MX Framework Context:** This layer powers **Stage 3 (Search & Compare)** with structured, queryable data. JSON-LD microdata at the pricing level enables agents to build accurate comparison tables without wasting tokens crawling your entire site.

### 4.1 The query-index.json

This file allows agents to perform Retrieval-Augmented Generation (RAG) without downloading your entire site.

**DDT Example Structure**:

```json
{
  "total": 91,
  "offset": 0,
  "limit": 91,
  "data": [
    {
      "path": "/blogs/ddt/integrations/ai-powered-eds",
      "title": "Building MX-Optimised Websites with Adobe EDS",
      "description": "Complete guide to architecting websites for AI agent interaction using Edge Delivery Services",
      "category": "Core AI/LLM Topics",
      "tags": ["AI", "EDS", "Architecture", "LLM"],
      "author": "Tom Cranstoun",
      "lastModified": 1735344000,
      "image": "/media/ai-native-architecture.png"
    },
    {
      "path": "/blogs/ddt/dev-guide",
      "title": "EDS Block Development Guide",
      "description": "Vanilla JavaScript patterns for creating high-performance custom blocks",
      "category": "Developer Documentation",
      "tags": ["EDS", "JavaScript", "Blocks", "Tutorial"],
      "author": "Tom Cranstoun",
      "lastModified": 1735257600,
      "image": "/media/block-development.png"
    }
  ]
}
```

### 4.2 Required Fields

| Field | Purpose | Example |
| ----- | ------- | ------- |
| `path` | Canonical URL path | `/blogs/category/article` |
| `title` | Page headline | "Complete Guide to X" |
| `description` | Summary for AI decision-making | "This page covers A, B, and C" |
| `category` | Primary classification | "Developer Guides" |
| `tags` | Filterable keywords | `["AI", "Tutorial", "Advanced"]` |
| `author` | Content creator | "Your Name" |
| `lastModified` | Unix timestamp | `1735344000` |
| `image` | Visual reference (optional) | `/media/image.png` |

### 4.3 Generating Your Index

**For Adobe EDS** (DDT's Platform):

See [code-examples/eds/helix-query.yaml](code-examples/eds/helix-query.yaml)

**For WordPress**:

**FIXED:** Uses `posts_per_page` instead of deprecated `numberposts`.

See [code-examples/wordpress/generate-query-index.php](code-examples/wordpress/generate-query-index.php)

**For Static Site Generators** (Hugo, Jekyll, Next.js):

See [code-examples/static-site/generate-index.js](code-examples/static-site/generate-index.js)

### 4.4 Enabling Filtered Queries

AI agents can now search your content:

**By Category**:

```http
GET /query-index.json?category=Developer+Documentation
```

**By Tags**:

```http
GET /query-index.json?tags=AI,Tutorial
```

**By Date**:

```http
GET /query-index.json?since=1735257600
```

Implement server-side filtering or use a static site plugin to handle these query parameters.

---

<a name="layer-5"></a>

## Layer 5: The Permission Layer (Robots.txt)

In 2025, `robots.txt` manages "inference rights" for AI agents, not just crawling permissions.

**MX Framework Context:** This layer controls which agents can access your site across **all 5 stages**. In 2025, robots.txt manages 'inference rights' for AI agents, not just crawling permissions.

### 5.1 The AI-Specific Configuration

**DDT's robots.txt**:

```text
# AI Agent Permissions (2025 Standards)

# OpenAI (ChatGPT, GPT-4, GPT Search)
User-agent: GPTBot
User-agent: OAI-SearchBot
Allow: /llms.txt
Allow: /ai-agents.md
Allow: /query-index.json
Allow: /blogs/ddt/
Disallow: /admin/
Disallow: /api/private/

# Anthropic (Claude)
User-agent: ClaudeBot
Allow: /llms.txt
Allow: /ai-agents.md
Allow: /query-index.json
Allow: /blogs/ddt/
Disallow: /admin/
Disallow: /api/private/

# Perplexity
User-agent: PerplexityBot
Allow: /llms.txt
Allow: /ai-agents.md
Allow: /query-index.json
Allow: /blogs/ddt/
Disallow: /admin/
Disallow: /api/private/

# Google Gemini (AI Training)
User-agent: google-extended
Allow: /llms.txt
Allow: /ai-agents.md
Allow: /query-index.json
Allow: /blogs/ddt/
Disallow: /admin/
Disallow: /api/private/

# Standard Crawlers (Google Search, Bing)
User-agent: Googlebot
User-agent: Bingbot
Allow: /
Disallow: /admin/
Disallow: /api/

# Default: Block Private Areas
User-agent: *
Disallow: /admin/
Disallow: /api/private/
Disallow: /internal-docs/
```

### 5.2 Understanding the Strategy

**Allow**: Explicitly permit access to AI manifests
**Disallow**: Protect proprietary business logic, admin panels, internal tools

**Critical**: Separate AI training permissions from search indexing. You can allow search bots while blocking AI model training by controlling `google-extended` and `GPTBot` separately.

### 5.3 Opting Out of AI Training

If you want to be indexed but NOT used for model training:

```text
User-agent: GPTBot
Disallow: /

User-agent: google-extended
Disallow: /

User-agent: Googlebot
Allow: /
```

---

<a name="layer-6"></a>

## Layer 6: The Authority Layer (Identity)

AI assistants prioritize verified entities. Establish trust through multiple identity signals.

**MX Framework Context:** This layer establishes credibility for **Stage 2 (Citation)** and **Stage 4 (Price Understanding)**. Schema.org structured data (Organisation, Product, Offer) ensures agents can confidently cite your expertise and extract accurate pricing.

### 6.1 HTML Meta Tags

**DDT Example**:

```html
<meta name="author" content="Tom Cranstoun, Digital Domain Technologies">
<meta name="description" content="Expert Adobe EDS consulting and AI integration resources">
<link rel="canonical" href="https://allabout.network/blogs/ddt/">
```

**Your Implementation**:

```html
<meta name="author" content="Your Name, Your Company">
<meta name="description" content="Your value proposition">
<link rel="canonical" href="https://yoursite.com/your-page/">
```

### 6.2 Schema.org Structured Data (JSON-LD)

**Standards Status:** Schema.org is an **established standard** with widespread support. Use with confidence—all major search engines and AI platforms recognise this markup.

AI agents parse Schema.org to understand your organisational identity.

**DDT's Organization Schema**:

```html
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Digital Domain Technologies",
  "alternateName": "DDT",
  "url": "https://allabout.network",
  "logo": "https://allabout.network/media/ddt-logo.png",
  "description": "Expert Adobe EDS consulting and AI integration",
  "founder": {
    "@type": "Person",
    "name": "Tom Cranstoun",
    "jobTitle": "Principal Consultant",
    "alumniOf": "Enterprise CMS Architecture (15+ years)",
    "sameAs": [
      "https://www.linkedin.com/in/tom-cranstoun/"
    ]
  },
  "sameAs": [
    "https://github.com/digitaldomaintech"
  ],
  "contactPoint": {
    "@type": "ContactPoint",
    "email": "info@digitaldomaintechnologies.com",
    "contactType": "Business Inquiries"
  }
}
</script>
```

**Template for Your Site**:

```html
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Your Company Name",
  "url": "https://yoursite.com",
  "logo": "https://yoursite.com/logo.png",
  "description": "Your company description",
  "founder": {
    "@type": "Person",
    "name": "Your Name",
    "jobTitle": "Your Title",
    "sameAs": [
      "https://linkedin.com/in/yourprofile"
    ]
  },
  "contactPoint": {
    "@type": "ContactPoint",
    "email": "your@email.com",
    "contactType": "Business Inquiries"
  }
}
</script>
```

### 6.3 The llms.txt Bio Section

Include a condensed bio in your `llms.txt`:

```markdown
## About the Author

**[Your Name]** is a [Your Primary Expertise] with [X] years of experience in [Your Industry]. Known for [Your Unique Value Proposition].

**Key Projects**: [Notable Project 1], [Notable Project 2], [Notable Project 3].

**Contact**: [your@email.com] | [Professional Profile Link]
```

### 6.4 Security Considerations

**Session Inheritance:** Browser-based AI agents (Claude, ChatGPT sidebar extensions) inherit your authenticated session. This means:

- Agents can access anything YOU can access when logged in
- No additional authentication challenge occurs
- Banks/services cannot distinguish agent activity from your activity

**Implications for MX-Optimised Sites:**

- Rate limit by user-agent if serving sensitive data (see [code-examples/nginx/rate-limiting.conf](code-examples/nginx/rate-limiting.conf))
- Consider delegation tokens for high-value operations
- Log agent activity separately for audit trails
- Review access logs regularly (see [code-examples/monitoring/server-log-analysis.sh](code-examples/monitoring/server-log-analysis.sh))

**Key insight from "The Invisible Users" book:** In-browser agents inherit proof-of-humanity tokens from the authenticated user's session. This is why banks cannot detect AI involvement—the agent inherits the user's authenticated session rather than failing to authenticate.

**Rate Limiting Configuration:** See [code-examples/nginx/rate-limiting.conf](code-examples/nginx/rate-limiting.conf) for implementation details.

---

<a name="layer-7"></a>

## Layer 7: The Interaction Layer (JavaScript Handshake)

For AI agents running in browsers (like sidebar assistants or extensions), implement a JavaScript handshake that injects instructions dynamically.

**MX Framework Context:** This layer enables **Stage 5 (Purchase Confidence)** by providing in-browser agents with dynamic instructions. JavaScript handshakes ensure explicit state visibility for checkout flows and form submissions.

### 7.1 The Detection Script

**FIXED:** Updated AI agent list for 2025 (includes DeepSeek-Bot, Gemini-Bot).

**For Standard HTML Sites:**

Add the vanilla JavaScript version before closing `</body>` tag (see WordPress implementation below for inline script).

**For WordPress:**

See [code-examples/wordpress/functions-headers.php](code-examples/wordpress/functions-headers.php) (includes both HTTP headers and AI handshake in `wp_footer`)

**For React/Next.js:**

See [code-examples/nextjs/AIHandshake.jsx](code-examples/nextjs/AIHandshake.jsx)

---

<a name="validation"></a>

## Validation and Maintenance

An MX-optimised site requires programmatic verification to prevent broken manifests.

### 8.1 The Verification Scripts

Two validation scripts are provided for different use cases:

**Simple Version** (Quick Health Check):

See [code-examples/validation/verify-ai-simple.js](code-examples/validation/verify-ai-simple.js)

- 30 lines of code
- Checks file accessibility only
- Fast execution
- Perfect for development

**Production Version** (Complete Validation):

See [code-examples/validation/verify-ai-production.js](code-examples/validation/verify-ai-production.js)

- 115 lines of code
- Content validation (checks for required headers and fields)
- Detailed error reporting
- CI/CD ready

**Usage:**

```bash
# Simple check
node code-examples/validation/verify-ai-simple.js

# Production check
node code-examples/validation/verify-ai-production.js
```

**Configuration:** Change the `DOMAIN` constant in either script to your domain before running.

### 8.2 CI/CD Integration

See [code-examples/validation/github-actions.yml](code-examples/validation/github-actions.yml)

Copy this file to `.github/workflows/ai-health-check.yml` in your repository to enable automated validation on every push, pull request, and daily at 2am UTC.

### 8.4 Monthly Maintenance Checklist

#### Week 1: Index Synchronization

- [ ] Run health check script
- [ ] Verify all paths in `query-index.json` return 200 status
- [ ] Check for dead links in `llms.txt`

#### Week 2: Content Updates

- [ ] Add new content to `llms.txt` if published
- [ ] Update `lastModified` timestamps in index
- [ ] Verify category and tag consistency

#### Week 3: Capability Review

- [ ] Review `ai-agents.md` for new services/skills
- [ ] Update endpoint documentation
- [ ] Verify citation formats are current

#### Week 4: Analytics

- [ ] Check server logs for AI bot traffic
- [ ] Identify most-accessed manifest files
- [ ] Monitor for 404s from AI user-agents

### 8.3 Monitoring AI Traffic

**Google Analytics 4:**

See [code-examples/monitoring/analytics-tracking.js](code-examples/monitoring/analytics-tracking.js) for complete AI agent detection and tracking code.

**Server-Side Log Analysis** (Apache/Nginx):

See [code-examples/monitoring/server-log-analysis.sh](code-examples/monitoring/server-log-analysis.sh) for a complete bash script that analyzes access logs for AI bot traffic patterns.

**Usage:**

```bash
./code-examples/monitoring/server-log-analysis.sh /var/log/nginx/access.log
```

---

<a name="checklist"></a>

## Implementation Checklist

Use this master checklist to track your MX transformation:

### Phase 1: Foundation

- [ ] **1.1** Configure HTTP Link headers for `/llms.txt` and `/ai-agents.md`
  - [ ] Test headers with `curl -I https://yoursite.com`
- [ ] **1.2** Update `robots.txt` with AI-specific permissions
  - [ ] Allow AI agents access to manifest files
  - [ ] Disallow private areas
- [ ] **1.3** Verify DNS and SSL certificate validity
  - [ ] Ensure HTTPS is enforced

### Phase 2: Knowledge Layer

- [ ] **2.1** Create `/llms.txt` at root domain
  - [ ] Write H1 brand title
  - [ ] Add blockquote value proposition
  - [ ] Create H2 section categories
  - [ ] Add 10-20 primary content links with descriptions
  - [ ] Include author/organisation bio
  - [ ] Add contact information
- [ ] **2.2** Validate Markdown formatting
  - [ ] Check for proper heading hierarchy
  - [ ] Verify all links are absolute URLs
  - [ ] Test rendering in Markdown preview

### Phase 3: Action Layer

- [ ] **3.1** Create `/ai-agents.md` at root domain
  - [ ] Define organisation identity
  - [ ] List 3-5 primary skills
  - [ ] Document endpoints for each skill
  - [ ] Specify permission levels
  - [ ] Define citation format
  - [ ] Add constraints section
- [ ] **3.2** Test endpoint accessibility
  - [ ] Verify each endpoint returns valid data

### Phase 4: Data Layer

- [ ] **4.1** Generate `/query-index.json`
  - [ ] Include all required fields (path, title, description, category, tags, author, lastModified)
  - [ ] Validate JSON syntax
  - [ ] Verify total count matches actual content
- [ ] **4.2** Set up automated regeneration
  - [ ] Configure build script or CMS hook
  - [ ] Test index updates when content changes
- [ ] **4.3** Implement query filtering (optional)
  - [ ] Add category filter support
  - [ ] Add tag filter support
  - [ ] Add date range filter support

### Phase 5: Authority Layer

- [ ] **5.1** Add HTML meta tags to all pages
  - [ ] author tag
  - [ ] description tag
  - [ ] canonical link
- [ ] **5.2** Implement Schema.org JSON-LD
  - [ ] Organization schema
  - [ ] Person schema (for founder/author)
  - [ ] ContactPoint schema
- [ ] **5.3** Validate structured data
  - [ ] Test with [Google Rich Results Test](https://search.google.com/test/rich-results)
  - [ ] Verify no errors or warnings

### Phase 6: Interaction Layer

- [ ] **6.1** Implement JavaScript AI handshake
  - [ ] Add detection script to all pages
  - [ ] Test meta tag injection
  - [ ] Configure analytics tracking
- [ ] **6.2** Set up AI traffic monitoring
  - [ ] Add server log analysis
  - [ ] Configure analytics events
  - [ ] Create dashboard for AI metrics

### Phase 7: Validation

- [ ] **7.1** Create health check script
  - [ ] Test accessibility of all manifest files
  - [ ] Validate Markdown structure
  - [ ] Validate JSON structure
  - [ ] Check for broken links
- [ ] **7.2** Set up CI/CD integration
  - [ ] Add GitHub Actions workflow
  - [ ] Configure automatic health checks
  - [ ] Set up failure notifications
- [ ] **7.3** Perform manual testing
  - [ ] Test from different AI platforms (ChatGPT, Claude, Perplexity)
  - [ ] Verify accurate content representation
  - [ ] Check citation format compliance

### Phase 8: Launch & Monitor

- [ ] **8.1** Submit to AI discovery services
  - [ ] Register with emerging agent directories
  - [ ] Submit to AI search engines
- [ ] **8.2** Establish monitoring routine
  - [ ] Regularly check health script results
  - [ ] Update content in manifests as needed
  - [ ] Review and expand skill definitions periodically
- [ ] **8.3** Document internal processes
  - [ ] Create team guide for maintaining AI manifests
  - [ ] Train content creators on metadata requirements
  - [ ] Establish review process for new content

---

## Real-World Results: The DDT MX Implementation

After implementing this MX architecture, Digital Domain Technologies (allabout.network) observed:

### Stage 1-2: Discovery & Citation Improvements

- **Before MX:** AI agents crawled 200+ pages to find specific EDS documentation (Stage 1 failure)
- **After MX:** Direct access via `/query-index.json?category=Developer+Documentation` (99% reduction in wasted tokens)
- **Citation Accuracy:** Consistent attribution as "Tom Cranstoun, Digital Domain Technologies" with verified credentials (Stage 2 success)
- **Hallucination Reduction:** AI no longer misattributed expertise to generic "Adobe documentation"

### Stage 3: Agent Traffic Growth

- **Month 1:** 47 AI bot visits (baseline measurement)
- **Month 3:** 312 AI bot visits (564% increase)
- **Most Active:** OAI-SearchBot (42%), ClaudeBot (31%), PerplexityBot (19%)

### Stage 4-5: Business Impact (Goal Completion)

- **Lead Quality:** 3x increase in "AI-referred" consulting inquiries (users asking specifically about services mentioned in AI responses)
- **Authority:** Featured as source in AI responses for searches like "Adobe EDS expert" and "AEM migration specialist"
- **First-Mover Advantage:** Agents return to DDT site for subsequent EDS queries (learned behaviour from successful Stage 5 completion)

**Key Insight:** MX implementation moved DDT through all 5 stages—from invisible (Stage 0) to trusted agent partner (Stage 5). Sites that complete the journey gain "agent's trust" and see compounding returns from repeat agent visits.

---

## Advanced Patterns

### 10.1 Multi-Language Support

For international audiences, create language-specific manifests:

```text
/llms.txt       (English, default)
/llms-es.txt    (Spanish)
/llms-fr.txt    (French)
/llms-de.txt    (German)
```

Update headers:

```http
Link: <https://yoursite.com/llms.txt>; rel="llms-txt"; hreflang="en",
      <https://yoursite.com/llms-es.txt>; rel="llms-txt"; hreflang="es"
```

### 10.2 Rate Limiting for AI Bots

Protect your server from aggressive crawling:

**Nginx**:

```nginx
# Limit AI bots to 10 req/min
limit_req_zone $http_user_agent zone=ai_bots:10m rate=10r/m;

location /query-index.json {
  if ($http_user_agent ~* "GPTBot|ClaudeBot|PerplexityBot") {
    limit_req zone=ai_bots burst=5;
  }
}
```

### 10.3 Versioned Manifests

Track changes to your AI configuration:

```text
/llms.txt           (current version)
/llms-v2.txt        (versioned for compatibility)
/ai-agents-v1.md    (legacy skills)
```

### 10.4 Dynamic Content Generation

For sites with thousands of pages, generate manifests dynamically:

**Express.js Example**:

```javascript
app.get('/query-index.json', async (req, res) => {
  const { category, tags, since } = req.query;

  let query = db.collection('posts');

  if (category) query = query.where('category', '==', category);
  if (tags) query = query.where('tags', 'array-contains-any', tags.split(','));
  if (since) query = query.where('lastModified', '>=', parseInt(since));

  const snapshot = await query.get();
  const data = snapshot.docs.map(doc => doc.data());

  res.json({
    total: data.length,
    offset: 0,
    limit: data.length,
    data
  });
});
```

---

## Common Mistakes to Avoid

### ❌ Mistake 1: Forgetting to Update llms.txt

**Impact**: AI agents reference outdated content
**Solution**: Add content updates to your publishing workflow

### ❌ Mistake 2: Blocking AI Bots in robots.txt

**Impact**: No AI can discover your manifest
**Solution**: Use explicit `Allow` directives for AI user-agents

### ❌ Mistake 3: Invalid JSON in query-index.json

**Impact**: Agents fail to parse your catalog
**Solution**: Use automated validation in CI/CD

### ❌ Mistake 4: Generic Descriptions

**Impact**: AI can't distinguish between similar pages
**Solution**: Write unique, specific descriptions for each entry

### ❌ Mistake 5: Missing Attribution Requirements

**Impact**: Your content is used without credit
**Solution**: Define citation format in ai-agents.md

### ❌ Mistake 6: Ignoring AI Traffic Analytics

**Impact**: You can't measure effectiveness or identify issues
**Solution**: Set up dedicated tracking for AI user-agents

### ❌ Mistake 7: Over-Optimizing for AI

**Impact**: Human visitors have poor experience
**Solution**: Maintain human-first design; AI optimization is supplemental

---

## Future-Proofing: 2026 and Beyond

**Important:** The following patterns are speculative and may not emerge as described. This section represents possibilities based on current trends, not confirmed roadmaps or established standards.

The MX-optimised web is rapidly evolving. Position your site for upcoming changes:

### Emerging Standards to Watch

#### 1. Agent Payment Protocol (AP2)

- **Purpose**: Allow AI agents to pay for premium content/services
- **Preparation**: Define pricing in `ai-agents.md`

#### 2. Federated Agent Directories

- **Purpose**: Central registries for discovering agent-capable sites
- **Preparation**: Ensure manifests are publicly accessible

#### 3. Multi-Modal Context

- **Purpose**: AI agents processing images, videos, and interactive content
- **Preparation**: Add media metadata to query-index.json

#### 4. Agentic Webhooks

- **Purpose**: Allow agents to subscribe to content updates
- **Preparation**: Design notification system in your architecture

### Staying Current

- **Subscribe**: Follow [AI Agent Standards Working Group](https://example.com) (emerging consortium)
- **Test Regularly**: Query your site with latest AI models monthly
- **Community**: Join discussions on MX architecture patterns
- **Iterate**: Update manifests as standards evolve

---

## Conclusion: Machine Experience (MX) is the Foundation

The transformation from a traditional website to an MX-optimised platform represents a fundamental shift in web architecture. By implementing the seven layers outlined in this guide—Network, Knowledge, Action, Data, Permission, Authority, and Interaction—you position your brand, content, and services for the agentic future.

### Key Takeaways

1. **Machine Experience (MX) prevents hallucination**—adding metadata and instructions so agents don't have to think reduces hallucination risk and enables accurate goal completion
2. **Miss any stage = catastrophic failure**—agents disappear silently with no analytics visibility and never return (unlike humans who persist through bad UX)
3. **MX is universal**—applies to ANY web goal (purchase, contact, inform, establish trust), not just ecommerce
4. **First-mover advantage is real**—agents learn which sites complete successfully and return for future tasks
5. **Design for the worst agent = universal compatibility**—explicit structure works for 100M parameter models AND frontier models (40% of agents have <100M parameters)
6. **Side benefit: accessibility improvements**—MX patterns also help human users with disabilities, but the primary focus is machine requirements
7. **Maintenance is ongoing**—MX manifests require regular updates to stay accurate

### The Digital Domain Technologies Example

Throughout this guide, you've seen how Tom Cranstoun and Digital Domain Technologies implemented these patterns on allabout.network. Their 91-post catalog of Adobe EDS expertise is now:

- **Discoverable**: AI agents find specific technical guidance in milliseconds
- **Authoritative**: Consistent attribution with verified credentials
- **Interactive**: Agents can query by category, tags, and recency
- **Maintainable**: Automated health checks prevent broken manifests

### Your MX Implementation Journey

**These MX patterns are universal.** Whether you run:

- A SaaS product documentation site
- An e-commerce platform
- A personal blog
- A corporate marketing site
- A news publication
- A consultant's portfolio

...you can prevent hallucination and enable agent goal completion by implementing:

1. **The 5-Stage MX Framework** (Discovery → Citation → Search & Compare → Price Understanding → Purchase Confidence)
2. **The 7-Layer Implementation** (Network, Knowledge, Action, Data, Permission, Authority, Interaction)
3. **Explicit structure for universal compatibility** (works for worst agent = works for all agents)

Start with Phase 1 of the implementation checklist and work through the phases systematically to transform your site into an MX-optimised platform that AI agents can discover, cite, compare, understand, and successfully complete tasks with—regardless of which of the 1M+ models is visiting your site.

**The wild west of AI models demands explicit structure. MX provides it.**

---

## Resources & Further Reading

### Official Standards

- [llms.txt Standard](https://llmstxt.org) - The original specification
- [Schema.org](https://schema.org) - Structured data vocabulary
- [Model Context Protocol (MCP)](https://github.com/anthropics/mcp) - Anthropic's agent integration standard

### MX Framework Resources

- **MX-Bible** - Comprehensive technical guide (~78,000 words) for developers and architects
- **MX-Handbook** - Quick reading guide for business users and tech leaders
- **14 Comprehensive Appendices** - Freely available online (~61,600 words):
  - Implementation guides, code examples, battle-tested patterns
  - Open access model: Start with free appendices, dive deeper with books
- **Web Audit Suite** - MX analysis tool for evaluating site compliance
- **Contact:** <info@digitaldomaintechnologies.com>

### DDT's Implementation

- **Live Site**: [allabout.network](https://allabout.network)
- **llms.txt**: [allabout.network/llms.txt](https://allabout.network/llms.txt)
- **Query Index**: [allabout.network/query-index.json](https://allabout.network/query-index.json)
- **Contact**: <info@digitaldomaintechnologies.com>

### Tools

- [Google Rich Results Test](https://search.google.com/test/rich-results) - Validate Schema.org
- [JSON Validator](https://jsonlint.com) - Check query-index.json syntax
- [cURL](https://curl.se) - Test HTTP headers

### Community

- [MX-Optimised Web Slack](https://example.com) - Join the discussion
- [GitHub Discussions](https://github.com/topics/ai-native-web) - Share implementations

---

## About This Guide

This complete blueprint was developed by Tom Cranstoun (Digital Domain Technologies) based on the real-world implementation of allabout.network—a 91-post Adobe EDS consultancy site transformed into a fully MX-optimised platform.

**Version**: 1.0 (December 2025)
**License**: Creative Commons BY-SA 4.0 (Attribution-ShareAlike)
**Attribution**: "Machine Experience (MX) Website Guide by Digital Domain Technologies (allabout.network)"

**Updates**: This guide will be updated quarterly as AI agent standards evolve. Check [allabout.network/blogs/ddt/ai-native-guide](https://allabout.network/blogs/ddt/ai-native-guide) for the latest version.

---

*Build for agents. Win the future.*
